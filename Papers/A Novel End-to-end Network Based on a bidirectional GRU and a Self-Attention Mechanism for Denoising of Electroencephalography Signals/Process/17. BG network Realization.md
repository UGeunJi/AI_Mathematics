### Attention 없이 BiGRU와 encoded layer까지 있는 걸 우선 오류가 없는지 모델을 구성해보려고 한다.

> 초기 dimension 문제로 오류나는 코드.

```py
import tensorflow as tf
from tensorflow.keras import datasets, layers, models, Input, Sequential

model = BG(datanum, encoded_dim)
def BG(datanum):
  model = tf.keras.Sequential()
  model.add(Input(shape=(datanum, 1)))
  model.add(layers.Bidirectional(layers.GRU(1, return_sequences = True)))
  
  
  # insert self-attention 
  
  model.add(layers.Bidirectional(layers.GRU(1, return_sequences = True)))



  model.add(layers.Flatten())

  model.add(layers.Dense(datanum))
  model.add(layers.ReLU())
  model.add(layers.Dropout(0.3))

  model.add(layers.Dense(datanum))
  model.summary()
  return model
```

---

<br>

> 수정 코드

`encoded layer 추가`

```py
import tensorflow as tf
from tensorflow.keras import datasets, layers, models, Input, Sequential

def BG(datanum, encoded_dim):
    model = tf.keras.Sequential()

    # Encoder Layer
    model.add(layers.Dense(encoded_dim, activation='relu', input_shape=(datanum, 1)))

    # Bidirectional GRU layers
    model.add(layers.Bidirectional(layers.GRU(1, return_sequences=True)))
    
    # insert self-attention 
    # (implement the self-attention mechanism here if needed)

    model.add(layers.Bidirectional(layers.GRU(1, return_sequences=True)))

    model.add(layers.Flatten())

    model.add(layers.Dense(datanum))
    model.add(layers.ReLU())
    model.add(layers.Dropout(0.3))

    model.add(layers.Dense(datanum))
    model.summary()
    return model
```

> 오류 여부 확인

![image](https://github.com/UGeunJi/AI_Papers-and-Mathematics/assets/84713532/8bdec8ec-b7ca-4075-a433-88f38326268c)

성공이다!!!!!!!!!!!!!!!!

이전에 왜 오류가 났던 건지 모르겠다..... <br>
main.py에서 enceoded_dim 계속 조정해주고 `model.add(layers.Bidirectional(layers.GRU(1, return_sequences=True)))`에서 <br>
1 대신 다른 값도 넣어볼 수도 있다. 다만, out of memory를 야기할 수 있다.

---

```
이제 여기에 논문에서 제시한 attention layer만 추가하면 된다.
논문에서는 Scaled Dot-Product Attention, Multi-Head Attention, Self-Attention을 활용해 layer를 구성했다.
```

오늘은 여기까지 하고 다음 작업은 스킵할까...ㅎ




