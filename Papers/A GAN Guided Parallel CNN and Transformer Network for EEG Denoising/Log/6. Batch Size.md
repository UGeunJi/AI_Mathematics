#### Batch size에 따라 best result가 달라지는 것을 확인해서 batch size가 무엇인지 정확히 짚고 넘어가고자 했다.

```
batch size는 1 epoch 동안 training data를 얼마나 나눠서 학습할지를 정해준다.

batch size가 클수록 train data를 한번에 많이 학습하기 때문에 정확한 gradient를 계산
하지만 local minimum에 갇히는 문제를 해결하지 못함

오히려 작은 batch size가 부정확한 gradient를 계산하게 되어 cost function 공간에서 오히려 local minimum을 뛰어넘음.
```

```
batch size가 클 때의 단점

크면 메모리 사용량이 많아져, 학습 속도가 느려질 수 있음
일부 작은 배치 사이즈에서만 나오는 노이즈나 변동성을 놓칠 수 있음
```

<br>

| batch size | noise | convergence | 장점 |
| --- | --- | --- | --- |
| 작을 때 | ↓ | ↑ | gpu를 최대한 활용 |
| 클 때 | ↑ | ↓ | overfitting을 방지 |

<br>

## Batch Normalization

입력값 x를 평균 0, 분산 1로 표준화하여 활성화 함수로 전달하고 활성화 함수 출력값의 분포를 고르게 해줌

x를 표준화하는 과정에서 배치 사이즈 단위로 평균과 분산을 계산  <br>
따라서 어떤 배치 사이즈를 선택하느냐에 따라 평균 분산이 달라져, 성능에 영향을 끼침

<br>

#### Rethinking 'Batch' in BatchNorm (FaceBook, 2021)

batch size는 32에서 128이 적절
