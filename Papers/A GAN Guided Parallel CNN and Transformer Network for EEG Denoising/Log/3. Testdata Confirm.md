이 network는 10fold 방식을 사용한다. <br>
처음엔 이 논문의 결과가 10fold의 best result를 뽑아내는 건줄 알았는데 아니었다. <br>
기존의 10fold 방식답게 각자 훈련을 시키고 거기서 평균값으로 결과를 도출해내는 것이었다.

#### 그럼 10fold에서 test data가 같은지, 모델들의 성능을 평가하기에 적절하게 짜여진 코드인지 확인해야 한다.

확인방법은 각 모델을 실행시키고 test data가 같은지 확인해보는 것이었다.

우선 fold마다 1epoch씩만 실행시켜서 결과를 얻어내야 했다.

## train_for_testset.py

```py
'''
Author: Yin Jin
Date: 2022-03-08 19:50:50
LastEditTime: 2023-04-24 19:02:10
LastEditors: JinYin
'''

import torch.nn.functional as F
import argparse, torch
from matplotlib.font_manager import weight_dict
import torch.optim as optim
import numpy as np
from tqdm import trange
from opts import get_opts
from audtorch.metrics.functional import pearsonr

import os
from models import *
from loss import denoise_loss_mse
from torch.utils.tensorboard import SummaryWriter
import torch
import matplotlib.pyplot as plt
import torch.nn as nn

from preprocess.DenoisenetPreprocess import *
from tools import pick_models

from torchsummary import summary as summary

loss_type = "feature+cls" 
if loss_type == "feature":
    w_c = 0
    w_f = 0.05      # 0, 0.01, 0.05, 0.1, 0.5
elif loss_type == "cls":
    w_c = 0.05      # 0, 0.001, 0.005, 0.01, 0.05
    w_f = 0
elif loss_type == "feature+cls":
    w_f = 0.05
    w_c = 0.05

def cal_SNR(predict, truth):
    if torch.is_tensor(predict):
        predict = predict.detach().cpu().numpy()
    if torch.is_tensor(truth):
        truth = truth.detach().cpu().numpy()

    PS = np.sum(np.square(truth), axis=-1)  # power of signal
    PN = np.sum(np.square((predict - truth)), axis=-1)  # power of noise
    ratio = PS / PN
    return torch.from_numpy(10 * np.log10(ratio))

def weights_init(m):
    if isinstance(m, nn.Conv1d):
        torch.nn.init.xavier_uniform_(m.weight)
        if m.bias is not None:
            torch.nn.init.zeros_(m.bias)
               
def train(opts, model, train_log_dir, val_log_dir, data_save_path, fold):
    if opts.noise_type == 'ECG':
        EEG_train_data, NOS_train_data, EEG_val_data, NOS_val_data, EEG_test_data, NOS_test_data = load_data_ECG(opts.EEG_path, opts.NOS_path, fold)
    elif opts.noise_type == 'EOG':
        EEG_train_data, NOS_train_data, EEG_val_data, NOS_val_data, EEG_test_data, NOS_test_data = load_data(opts.EEG_path, opts.NOS_path, fold)
    elif opts.noise_type == 'EMG':
        EEG_train_data, NOS_train_data, EEG_val_data, NOS_val_data, EEG_test_data, NOS_test_data = load_data(opts.EEG_path, opts.NOS_path, fold)
    elif opts.noise_type == 'Hybrid':
        EMG_path = "./data/EMG_shuffle.npy"
        EOG_path = "./data/EOG_shuffle.npy"
        EEG_train_data, NOS_train_data, EEG_val_data, NOS_val_data, EEG_test_data, NOS_test_data = load_data_hybrid(opts.EEG_path, EMG_path, EOG_path, fold)
    train_data = EEGwithNoise(EEG_train_data, NOS_train_data, opts.batch_size)
    val_data = EEGwithNoise(EEG_val_data, NOS_val_data, opts.batch_size)
    test_data = EEGwithNoise(EEG_test_data, NOS_test_data, opts.batch_size)
    
    model_d = Discriminator().to('cuda:0')
    
    model_d.apply(weights_init)
    model.apply(weights_init)

    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.5, 0.9), eps=1e-8)
    optimizer_D = torch.optim.Adam(model_d.parameters(), lr=0.0001)
        
    best_mse = 100
    if opts.save_result:
        train_summary_writer = SummaryWriter(train_log_dir)
        val_summary_writer = SummaryWriter(val_log_dir)
        f = open(data_save_path + "result.txt", "a+")
    
    for epoch in range(opts.epochs):
        model.train()
        model_d.train()
        losses = []
        for batch_id in trange(train_data.len()):
            x_t, y_t = train_data.get_batch(batch_id)
            x_t, y_t = torch.Tensor(x_t).to(opts.device).unsqueeze(dim=1), torch.Tensor(y_t).to(opts.device)
            
            y_original = y_t
            if batch_id % 1 == 0:
                p_t = model(x_t).view(x_t.shape[0], -1)
                fake_y, _, _, _ = model_d(p_t.unsqueeze(dim=1))
                real_y, _, _, _ = model_d(y_t.unsqueeze(dim=1))
                
                d_loss = 0.5 * (torch.mean((fake_y) ** 2)) + 0.5 * (torch.mean((real_y - 1) ** 2))
                
                optimizer_D.zero_grad()
                d_loss.backward()
                optimizer_D.step()
            
            if batch_id % 1 == 0:
                p_t = model(x_t).view(x_t.shape[0], -1)

                fake_y, _, fake_feature2, _ = model_d(p_t.unsqueeze(dim=1))
                _, _, true_feature2, _ = model_d(y_t.unsqueeze(dim=1))

                y_t = y_original
                loss = denoise_loss_mse(p_t, y_t)
                
                if loss_type == "cls":
                    g_loss = loss + w_c * (torch.mean((fake_y - 1) ** 2)) 
                elif loss_type == "feature": 
                    g_loss = loss + w_f * denoise_loss_mse(fake_feature2, true_feature2)
                elif loss_type == "feature+cls": 
                    g_loss = loss + w_f * denoise_loss_mse(fake_feature2, true_feature2) + w_c * (torch.mean((fake_y - 1) ** 2))  

                optimizer_D.zero_grad()
                optimizer.zero_grad()
                g_loss.backward()
                optimizer.step()
                    
                losses.append(g_loss.detach())
                
        train_data.shuffle()
        train_loss = torch.stack(losses).mean().item()

        if opts.save_result:
            train_summary_writer.add_scalar("Train loss", train_loss, epoch)
        
        # val
        model.eval()
        losses = []
        for batch_id in range(val_data.len()):
            x_t, y_t = val_data.get_batch(batch_id)
            x_t, y_t = torch.Tensor(x_t).to(opts.device).unsqueeze(dim=1), torch.Tensor(y_t).to(opts.device)
            
            with torch.no_grad():
                p_t = model(x_t).view(x_t.shape[0], -1)
                loss = ((p_t - y_t) ** 2).mean(dim=-1).sqrt().detach()
                losses.append(loss)
        val_mse = torch.cat(losses, dim=0).mean().item()
        val_summary_writer.add_scalar("Val loss", val_mse, epoch)
        
        # test
        model.eval()
        losses = []
        single_acc, single_snr = [], []
        clean_data, output_data, input_data = [], [], []
        correct_d, sum_d = 0, 0

        test_tensor_xt, test_tensor_yt = [], []

        for batch_id in range(test_data.len()):
            x_t, y_t = test_data.get_batch(batch_id)
            x_t, y_t = torch.Tensor(x_t).to(opts.device).unsqueeze(dim=1), torch.Tensor(y_t).to(opts.device)
            
            #print('x_t:', x_t)
            #print('y_t:', y_t)

            test_tensor_xt.extend(x_t.cpu().numpy())
            test_tensor_yt.extend(y_t.cpu().numpy())

            with torch.no_grad():
                p_t = model(x_t).view(x_t.shape[0], -1)
                loss = (((p_t - y_t) ** 2).mean(dim=-1).sqrt() / (y_t ** 2).mean(dim=-1).sqrt()).detach()
                losses.append(loss.detach())
                single_acc.append(pearsonr(p_t, y_t))
                single_snr.append(cal_SNR(p_t, y_t))
                
                p_t = model(x_t).view(x_t.shape[0], -1)
                
                fake_y, _, _, _ = model_d(p_t.unsqueeze(dim=1))
                real_y, _, _, _ = model_d(y_t.unsqueeze(dim=1))
                
                correct_d += torch.sum(torch.where(fake_y < 0.5, 1, 0)) + torch.sum(torch.where(real_y > 0.5, 1, 0))
                sum_d += p_t.shape[0] * 2
                    
            output_data.append(p_t.cpu().numpy()), clean_data.append(y_t.cpu().numpy()), input_data.append(x_t.cpu().numpy())
                    
        test_rrmse = torch.cat(losses, dim=0).mean().item()
        sum_acc = torch.cat(single_acc, dim=0).mean().item()
        sum_snr = torch.cat(single_snr, dim=0).mean().item()
        
        val_summary_writer.add_scalar("test rrmse", test_rrmse, epoch)
        
        np.save(f"{data_save_path}/test_tensor_xt.npy", np.array(test_tensor_xt))
        np.save(f"{data_save_path}/test_tensor_yt.npy", np.array(test_tensor_yt))
        torch.save(model, f"{data_save_path}/best_{opts.denoise_network}.pth")


if __name__ == '__main__':
    opts = get_opts()
    np.random.seed(0)
    opts.epochs = 1        # 50 200
    opts.depth = 6
    opts.noise_type = 'EOG'     # EMG EOG Hybrid
    opts.denoise_network = 'NovelCNN'

    opts.EEG_path = "./data/EEG_shuffle.npy"
    opts.NOS_path = f"./data/{opts.noise_type}_shuffle.npy"
    opts.save_path = "./test_data_confirm/{}/{}/".format(opts.noise_type, opts.denoise_network)

    for fold in range(10):
        print(f"fold:{fold}")
        model = pick_models(opts, data_num=512, embedding=1)
        print(opts.denoise_network)
        #summary(model, (3400,512))
        
        foldername = '{}_{}_{}_{}_{}_{}'.format(opts.denoise_network, opts.noise_type, opts.epochs, fold, w_c, w_f)
            
        train_log_dir = opts.save_path +'/'+foldername +'/'+ '/train'
        val_log_dir = opts.save_path +'/'+foldername +'/'+ '/test'
        data_save_path = opts.save_path +'/'+foldername +'/'
        
        if not os.path.exists(train_log_dir):
            os.makedirs(train_log_dir)
        
        if not os.path.exists(val_log_dir):
            os.makedirs(val_log_dir)
        
        if not os.path.exists(data_save_path):
            os.makedirs(data_save_path)

        train(opts, model, train_log_dir, val_log_dir, data_save_path, fold)
```

#### 이로써 FCNN, SimpleCNN, NovelCNN의 test data를 얻어냈다.

이제 같은지 확인해봐야 한다.

3개의 코드가 있는데 데이터만 다르므로 하나만 올리겠다.

## testdata_confirm_hybrid.py

```py
import argparse, torch
from matplotlib.font_manager import weight_dict
import torch.optim as optim
import numpy as np
from tqdm import trange
from opts import get_opts
from audtorch.metrics.functional import pearsonr

import os
from models import *
from loss import denoise_loss_mse
from torch.utils.tensorboard import SummaryWriter
import torch
import matplotlib.pyplot as plt
import torch.nn as nn

from preprocess.DenoisenetPreprocess import *
from tools import pick_models

from torchsummary import summary as summary

for i in range(10):
    print(f'EMG test dataset Concordance Rate Fold {i}')

    FCNN_xt = np.load(f"./test_data_confirm/EMG/FCNN/FCNN_EMG_1_{i}_0.05_0.05/test_tensor_xt.npy")
    FCNN_yt = np.load(f"./test_data_confirm/EMG/FCNN/FCNN_EMG_1_{i}_0.05_0.05/test_tensor_yt.npy")
    SimpleCNN_xt = np.load(f"./test_data_confirm/EMG/SimpleCNN/SimpleCNN_EMG_1_{i}_0.05_0.05/test_tensor_xt.npy")
    SimpleCNN_yt = np.load(f"./test_data_confirm/EMG/SimpleCNN/SimpleCNN_EMG_1_{i}_0.05_0.05/test_tensor_yt.npy")
    NovelCNN_xt = np.load(f"./test_data_confirm/EMG/NovelCNN/NovelCNN_EMG_1_{i}_0.05_0.05/test_tensor_xt.npy")
    NovelCNN_yt = np.load(f"./test_data_confirm/EMG/NovelCNN/NovelCNN_EMG_1_{i}_0.05_0.05/test_tensor_yt.npy")

    # print(FCNN_xt.shape)
    # print(FCNN_yt.shape)
    # print(SimpleCNN_xt.shape)
    # print(SimpleCNN_yt.shape)
    # print(NovelCNN_xt.shape)
    # print(NovelCNN_yt.shape)

    print('xt FCNN vs SimpleCNN:', np.all(FCNN_xt == SimpleCNN_xt))
    print('xt SimpleCNN vs NovelCNN:', np.all(SimpleCNN_xt == NovelCNN_xt))
    print('xt NovelCNN vs FCNN:', np.all(NovelCNN_xt == FCNN_xt))

    print('yt FCNN vs SimpleCNN:', np.all(FCNN_yt == SimpleCNN_yt))
    print('yt SimpleCNN vs NovelCNN:', np.all(SimpleCNN_yt == NovelCNN_yt))
    print('yt NovelCNN vs FCNN:', np.all(NovelCNN_yt == FCNN_yt))
```

모듈은 지우기 귀찮아서 저렇게 불필요하게 많은 것이다. numpy빼고 다 지워도 된다.

### 결과 이미지

<img width="211" alt="image" src="https://github.com/UGeunJi/AI_Papers-and-Mathematics/assets/84713532/512ea6ce-9329-4462-a8ff-65f81907d561">

<img width="192" alt="image" src="https://github.com/UGeunJi/AI_Papers-and-Mathematics/assets/84713532/ef34a968-95a0-4b41-939f-d26bdd93c557">

<img width="195" alt="image" src="https://github.com/UGeunJi/AI_Papers-and-Mathematics/assets/84713532/4b51f58b-1f89-4912-a686-cef8289c0a29">

이해를 위해 짧게 설명을 하자면, data도 3개(EOG, EMG, Hybrid) / model도 3개(FCNN, SimpleCNN, NovelCNN)다.

각 결과 이미지 우선 data 별로 나눴고, 이미지 내에서는 0~9까지의 fold로 되어있으며 각 fold 내에는 모델 간 같은지를 비교한 결과가 나와있다. <br>
np.all() function을 활용했기 때문에 하나라도 틀리면 False가 나오게 되어있다. <br>
하지만 보이는 것과 같이 모두 True로 나와서 testdata는 모두 같은 것을 쓰고 있다는 것을 확인할 수 있다.
