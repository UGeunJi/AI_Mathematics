# ONTO 전사(전체를 사용) 함수

#### 정의역보다 공역이 원소의 개수가 많아야함 => onto(전사)가 될 수 있는 가능성 UP

<br>

$$ 공역 ~~ = ~~ 치역 ~~~ T: ~~ R^3 \rightarrow R^2$$

<br>

$$ Definition: ~~ A ~~ mapping ~~ T: R^n \rightarrow R^m ~~ is ~~ said ~~ to ~~ be ~~ ONTO ~~ R^m ~~ if ~~ each ~~ b \in ~~ R^m ~~ is ~~ the ~~ image ~~ of ~~ at ~~ least ~~ one ~~ x \in ~~ R^n.$$

$$ That ~~ is, ~~ the ~~ range ~~ is ~~ equal ~~ to ~~ the ~~ co-domain.$$

![image](https://github.com/UGeunJi/AI_Papers-and-Mathematics/assets/84713532/15367ff1-119b-44ba-a286-73cbcb263e90)

$$T: R^2 \rightarrow R^3 ~~~~~~~~~ T\bigg(\begin{bmatrix} 1 \ 
                                                          3 \\ \end{bmatrix} \bigg) = \begin{bmatrix} 1 \\
                                                                                                      2 \\
                                                                                                      3 \\ \end{bmatrix} ~~~~~~~~ T\bigg(\begin{bmatrix} 0 \\
                                                                                                                                                         1 \\ \end{bmatrix} \bigg) = \begin{bmatrix} 4 \\
                                                                                                                                                                                                     5 \\
                                                                                                                                                                                                     6 \\ \end{bmatrix}$$
                                                                                                                                                                                                     
#### Neural Network에서의 예시

$$ ex) ~~ R^3 \rightarrow R^2 $$

<br>

차원을 줄이는 경우에 공역이 정의역보다 적어서 중복이 되면 안된다. Linearly Dependent하다.

차원이 줄어듦으로써 정보손실이 일어남. But, 중요한 정보만을 취함.

<br>

$$Linear + Non-Linear: ~~ 평면의 ~~ 곡선까지 ~~ 고려한 ~~ 데이터화 ~~~~~ \rightarrow ~~~~~ \therefore Manifold ~~ Learning: ~~ 실제 ~~ 데이터가 ~~ 존재할 ~~ 법한 ~~ 서브공간$$

<br>

GAN을 예시로 들면, 차원을 줄이면서 필요한 정보만을 가지게 된다(Encoding). 그리고 Decoding의 과정을 거치면서 이를 필요한 정보만 취합된 부분을 극대화시킨다.

<br>

### Linearly dependent

$$T \Bigg(\begin{bmatrix} 1 \\
                           0 \\
                           0 \\ \end{bmatrix}\Bigg) = \begin{bmatrix} 1 \\
                                                                2 \\ \end{bmatrix} ~~~~~ T \bigg(\begin{bmatrix} 0 \\
                                                                                                                 1 \\
                                                                                                                 0 \\ \end{bmatrix} \Bigg) = \begin{bmatrix} 3 \\
                                                                                                                                                      5 \\ \end{bmatrix} ~~~~~~ T \Bigg(\begin{bmatrix} 1 \\
                                                                                                                                                                                                        0 \\
                                                                                                                                                                                                        0 \\ \end{bmatrix}\Bigg) = \begin{bmatrix} 5 \\
                                                                                                                                                                                                                                            8 \\ \end{bmatrix}$$

<br>

$$T:y ~~ = ~~ Ax ~~ = ~~ \begin{bmatrix} 1&3&5 \\
                                         2&5&8 \\ \end{bmatrix} \begin{bmatrix} x_1 \\
                                                                                x_2 \\
                                                                                x_3 \\ \end{bmatrix}$$

---

# One-to-One: 1대 1 함수

$$Definition: ~~ A ~~ mapping ~~ T: ~~ R^n \rightarrow R^m ~~ is ~~ said ~~ to ~~ be ~~ one-to-one ~~ if ~~ each ~~ be \in R^m ~~ is ~~ the ~~ image ~~ of ~~ at ~~ least ~~ one ~~ x \in R^n$$

$$That ~~ is, ~~ each ~~ output ~~ vector ~~ in ~~ the ~~ range ~~ is ~~ mapped ~~ by ~~ only ~~ one ~~ input ~~ vector, ~~ no ~~ more ~~ than ~~ that.$$

$$\begin{bmatrix} 1&3&5 \\
                  2&4&8 \\ \end{bmatrix} \begin{bmatrix} x_1 \\
                                                         x_2 \\
                                                         x_3 \\ \end{bmatrix} = \begin{bmatrix} 5 \\
                                                                                                7 \\ \end{bmatrix} ~~~ \Rightarrow ~~~ Ax = b$$
    
<br>

역함수를 구하는 과정 = Linear Regression을 구하는 과정.

<br>

$$ b \in span\begin{Bmatrix} \begin{bmatrix} 1 \\
                              2 \\ \end{bmatrix} ~~ \begin{bmatrix} 3 \\
                                                                    4 \\ \end{bmatrix} ~~ \begin{bmatrix} 5 \\
                                                                                                          8 \\ \end{bmatrix} \end{Bmatrix}$$

1대 1 함수냐 = Linearly dependent냐 independent냐
dependent = 중복 / independent = 동치

---

# Neural Network Example

- Fully-connected layers

# pdf 예시 사진

$$x_1 - x_2 + 0 \cdot x_3$$

$$ height - weight ~~ 170 - 70 and 180 - 80 (2 dimension) ~~~~~ \Rightarrow ~~~~~ over-weighted 100 (1dimension)$$

#### 정보손실의 관계: 역함수로써 정보를 복원할 수 없다. But, 중요한 정보만을 취함.

<br>

$$\therefore ~~~ ONTO and ONE-TO-ONE$$

$$Let T: R^n \rightarrow R^m ~~ be ~~ a ~~ linear ~~ transformation, ~~ i.e., T(x) = Ax for all x \in R^n$$

$$T ~~ is ~~ one-to-one ~~ if ~~ and ~~ only ~~ if ~~ the ~~ columns ~~ of ~~ A ~~ are ~~ linearly ~~ independent.$$

$$T ~~ maps ~~ R^n ~~ ONTO R^m ~~ if ~~ and ~~ only ~~ if ~~ the ~~ columns ~~ of ~~ A ~~ span ~~ R^m$$

# 띄어쓰기 수정하기
